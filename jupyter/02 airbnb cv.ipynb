{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data set used in this example is from http://archive.ics.uci.edu/ml/datasets/Wine+Quality\n",
    "# P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis.\n",
    "# Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(40)\n",
    "\n",
    "# Read the wine-quality csv file (make sure you're running this from the root of MLflow!)\n",
    "data = pd.read_csv(\"../data/airbnb-cleaned-mlflow.csv\").iloc[3:]\n",
    "\n",
    "# Split the data into training and test sets. (0.75, 0.25) split.\n",
    "train, test = train_test_split(data)\n",
    "\n",
    "# The predicted column is \"quality\" which is a scalar from [3, 9]\n",
    "train_x = train.drop([\"price\"], axis=1)\n",
    "test_x = test.drop([\"price\"], axis=1)\n",
    "train_y = train[[\"price\"]]\n",
    "test_y = test[[\"price\"]]\n",
    "\n",
    "alpha = 1.0\n",
    "l1_ratio = 0.1\n",
    "\n",
    "lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metrics(actual, pred):\n",
    "    rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "    mae = mean_absolute_error(actual, pred)\n",
    "    r2 = r2_score(actual, pred)\n",
    "    return rmse, mae, r2\n",
    "\n",
    "predicted_qualities = lr.predict(test_x)\n",
    "\n",
    "(rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)\n",
    "\n",
    "print(\"Elasticnet model (alpha=%f, l1_ratio=%f):\" % (alpha, l1_ratio))\n",
    "print(\"  RMSE: %s\" % rmse)\n",
    "print(\"  MAE: %s\" % mae)\n",
    "print(\"  R2: %s\" % r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation via local GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'alpha':(1.0, 0.5, 0.1), 'l1_ratio':[0.1, 0.5, 1.0]}\n",
    "cv = GridSearchCV(lr, parameters, cv=5, return_train_score=True)\n",
    "cv.fit(train_x, train_y)                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log result of grid search to Tracking Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import os\n",
    "\n",
    "def track(clf, name):\n",
    "    params = list(clf.param_grid.keys())\n",
    "    cv_results = clf.cv_results_\n",
    "    rank = cv_results[\"rank_test_score\"]\n",
    "\n",
    "    for i in range(len(rank)):\n",
    "        if i == clf.best_index_:\n",
    "            run_name = \"run %d (best run):\" % i\n",
    "        else:\n",
    "            run_name = \"run %d:\" % i\n",
    "        print(run_name)\n",
    "        with mlflow.start_run(run_name=run_name, nested=True) as run:\n",
    "            mlflow.log_param(\"folds\", clf.cv)\n",
    "            for param in params:\n",
    "                print(\"  -\", param, cv_results[\"param_%s\" % param][i])\n",
    "                mlflow.log_param(param, cv_results[\"param_%s\" % param][i])\n",
    "    \n",
    "            mlflow.log_metric(\"rank_test_score\" , cv_results[\"rank_test_score\"][i])\n",
    "            mlflow.log_metric(\"mean_train_score\", cv_results[\"mean_train_score\"][i])\n",
    "            mlflow.log_metric(\"std_train_score\",  cv_results[\"std_train_score\"][i])\n",
    "            mlflow.log_metric(\"mean_test_score\",  cv_results[\"mean_test_score\"][i])\n",
    "            mlflow.log_metric(\"std_test_score\",   cv_results[\"std_test_score\"][i])\n",
    "            if i == clf.best_index_:\n",
    "                mlflow.sklearn.log_model(cv.best_estimator_, \"model\")\n",
    "                local_path = os.path.join(\".\", \"%s\" % name)\n",
    "                local_csv = os.path.join(local_path, \"cv_results.csv\")\n",
    "                if not os.path.exists(local_path):\n",
    "                    os.mkdir(local_path)\n",
    "                pd.DataFrame(cv_results).sort_values(by='rank_test_score').to_csv(local_csv, index=False)\n",
    "                mlflow.log_artifact(local_csv, \"cv_results\")\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Local tracking server**\n",
    "\n",
    "```bash\n",
    "cd /opt/mlflow-tracking-server/\n",
    "mkdir -p backend\n",
    "mkdir -p artifacts\n",
    "mlflow server --backend-store-uri ./backend --default-artifact-root ./artifacts/  --host 0.0.0.0\n",
    "```\n",
    "\n",
    "**In the project folder**\n",
    "\n",
    "```bash\n",
    "ln -s /opt/mlflow-tracking-server/artifacts artifacts\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"http://0.0.0.0:5000\")\n",
    "\n",
    "experiment=\"airbnb-jupyter\"\n",
    "mlflow.set_experiment(experiment)\n",
    "\n",
    "track(cv, \"airbnb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlflow.set_tracking_uri(\"databricks://westeu\")\n",
    "\n",
    "#experiment=\"/Shared/experiments/airbnb-jupyter\"\n",
    "#mlflow.set_experiment(experiment)\n",
    "\n",
    "#track(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
